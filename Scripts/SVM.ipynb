{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "325bdd60-3815-4e13-979d-8b98ccac924b",
   "metadata": {},
   "source": [
    "# Data 245 - Machine Learning Project \n",
    "\n",
    "# Internet Downtime Prediction Analysis using ML Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132de6c3-3375-4291-834d-7ab16d21d6fc",
   "metadata": {},
   "source": [
    "### Presented By: Group 6 (Bhavik Patel, Poojan Gagrani, Kashish Thakur, Yuti Khamker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1293c8f3-d237-40f4-b40d-69880377da7a",
   "metadata": {},
   "source": [
    "## 1. Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecf2fb4-e09d-4fb2-a84a-30bed2c1f63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from io import StringIO\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.utils import resample, shuffle\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56134e8d-e992-4f49-8b59-2c2ba80b729f",
   "metadata": {},
   "source": [
    "## 2. Reading file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c079f776-660d-4e3d-9fd3-c1ea89ca8e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/bhavikpatel/Desktop/poject MSDA/Data 245/Project/Data/Outage_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f29efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(n=1000000, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095b834b-c7e6-4783-9b60-c91c088c988b",
   "metadata": {},
   "source": [
    "## 3. Data understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adc481d-e7b9-47e5-b90d-c3e8bde4f9b8",
   "metadata": {},
   "source": [
    "### Original feature description from data source, Ref: https://wiki.mozilla.org/Mozilla_Network_Outages_Data_Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c0d09b-1a64-487d-847c-265fb68fc446",
   "metadata": {},
   "source": [
    "`country`: the Country code of the client.\n",
    "\n",
    "`city`: the City name (only for cities with a population >= 15000, 'unknown' otherwise).\n",
    "\n",
    "`datetime`: the date and the time (truncated to hour) the data was submitted by the client.\n",
    "\n",
    "`proportion_undefined`: the proportion of users who failed to send telemetry for a reason that was not listed in the other cases.\n",
    "\n",
    "`proportion_timeout`: the proportion of users that had their connection timeout while uploading telemetry (after 90s, in Firefox Desktop).\n",
    "\n",
    "`proportion_abort`: the proportion of users that had their connection terminated by the client (for example, terminating open connections before shutting down).\n",
    "\n",
    "`proportion_unreachable`: the proportion of users that failed to upload telemetry because the server was not reachable (e.g. because the host was not reachable, proxy problems or OS waking up after a suspension).\n",
    "\n",
    "`proportion_terminated`: the proportion of users that had their connection terminated internally by the networking code.\n",
    "\n",
    "`proportion_channel_open`: the proportion of users for which the upload request was terminated immediately, by the client, because of a Necko internal error.\n",
    "\n",
    "`avg_dns_success_time`: the average time it takes for a successful DNS resolution, in milliseconds.\n",
    "\n",
    "`missing_dns_success`: counts how many sessions did not report the `DNS_LOOKUP_TIME` histogram.\n",
    "\n",
    "`avg_dns_failure_time`: the average time it takes for an unsuccessful DNS resolution, in milliseconds.\n",
    "\n",
    "`missing_dns_failure`: counts how many sessions did not report the `DNS_FAILED_LOOKUP_TIME` histogram.\n",
    "\n",
    "`count_dns_failure`: the average count of unsuccessful DNS resolutions reported.\n",
    "\n",
    "`ssl_error_prop`: the proportion of users that reported an error through the `SSL_CERT_VERIFICATION_ERRORS` histogram.\n",
    "\n",
    "`avg_tls_handshake_time`: the average time after the TCP SYN to ready for HTTP, in milliseconds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0968121d-1250-4a61-99f2-000c76f1329d",
   "metadata": {},
   "source": [
    "### Defining dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1565e4b8-5434-44e4-ac58-10b039978c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86318d29-73b6-45fe-96b2-727febd99b74",
   "metadata": {},
   "source": [
    "**Description**\n",
    "\n",
    "Showing first 5 values of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9500cc3f-c461-467b-afee-186c6f51c7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e8fbc2-35cf-4f96-9e07-1b375eb7c5a3",
   "metadata": {},
   "source": [
    "**Description**\n",
    "\n",
    "Showing last 5 values of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b851d8cf-9fd9-4b84-997d-62569bd44b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985b05ec-4e71-46a8-8917-14c782e7a917",
   "metadata": {},
   "source": [
    "**Description**\n",
    "\n",
    "Checking datatypes of the attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc4e58b-c3e7-49a5-8175-0a0799933531",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3d0ab3-5a71-4c7b-ae20-18d66a4739b6",
   "metadata": {},
   "source": [
    "**Description**\n",
    "\n",
    "Showing descriptive statistics of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86567243-1eeb-48c4-aa7b-426127bfe4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027feece-991d-408b-8207-75af978be7b2",
   "metadata": {},
   "source": [
    "**Description**\n",
    "\n",
    "Showing unique values of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3058640c-c45c-424b-b1c4-6b089c078e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabced47-b9bd-4da2-baaf-ea93c50ad2d7",
   "metadata": {},
   "source": [
    "**Description**\n",
    "\n",
    "Checking the null values in the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa4dcd8-f2c9-430a-85e3-93f63ada7cee",
   "metadata": {},
   "source": [
    "## 4. Data Quality Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e04cbb-962b-410f-a61f-ee6978a0cf9f",
   "metadata": {},
   "source": [
    "### Data quality for continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f23f4fe-fb8f-4c5a-ad12-fddc168e46d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_features = [\n",
    "    'proportion_undefined', 'proportion_timeout', 'proportion_abort',\n",
    "    'proportion_unreachable', 'proportion_terminated', 'proportion_channel_open',\n",
    "    'avg_dns_success_time', 'avg_dns_failure_time', 'count_dns_failure',\n",
    "    'ssl_error_prop', 'avg_tls_handshake_time'\n",
    "]\n",
    "\n",
    "data_quality_report = pd.DataFrame(index=continuous_features)\n",
    "\n",
    "data_quality_report['Count'] = df[continuous_features].count()\n",
    "\n",
    "data_quality_report['Missing Values in %'] = (1 - (df[continuous_features].count() / len(df))) * 100\n",
    "\n",
    "data_quality_report['Cardinality'] = df[continuous_features].nunique()\n",
    "\n",
    "data_quality_report['Minimum'] = df[continuous_features].min()\n",
    "\n",
    "data_quality_report['Quartile 1'] = df[continuous_features].quantile(0.25)\n",
    "\n",
    "data_quality_report['Mean'] = df[continuous_features].mean()\n",
    "\n",
    "data_quality_report['Median'] = df[continuous_features].median()\n",
    "\n",
    "data_quality_report['Quartile 3'] = df[continuous_features].quantile(0.75)\n",
    "\n",
    "data_quality_report['Maximum'] = df[continuous_features].max()\n",
    "\n",
    "data_quality_report['Standard Deviation'] = df[continuous_features].std()\n",
    "\n",
    "data_quality_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b631aa96-4ba7-45f9-90de-5a79a47a101f",
   "metadata": {},
   "source": [
    "### Data quality for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e324792-ad73-4ea0-94a5-2f386cd665a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['country', 'city']\n",
    "\n",
    "data_quality_report_categorical = pd.DataFrame(index=categorical_features)\n",
    "\n",
    "data_quality_report_categorical['Count'] = df[categorical_features].count()\n",
    "\n",
    "data_quality_report_categorical['Missing Values in %'] = (1 - (df[categorical_features].count() / len(df))) * 100\n",
    "\n",
    "data_quality_report_categorical['Cardinality'] = df[categorical_features].nunique()\n",
    "\n",
    "data_quality_report_categorical['Mode'] = df[categorical_features].mode().iloc[0]\n",
    "\n",
    "data_quality_report_categorical['Mode Frequency'] = df[categorical_features].apply(lambda x: x.value_counts().iloc[0])\n",
    "\n",
    "data_quality_report_categorical['Mode in %'] = (df[categorical_features].apply(lambda x: x.value_counts().iloc[0]) / len(df)) * 100\n",
    "\n",
    "data_quality_report_categorical['2nd Mode'] = df[categorical_features].apply(lambda x: x.value_counts().index[1] if len(x.value_counts()) > 1 else 'N/A')\n",
    "\n",
    "data_quality_report_categorical['2nd Mode Frequency'] = df[categorical_features].apply(lambda x: x.value_counts().iloc[1] if len(x.value_counts()) > 1 else 'N/A')\n",
    "\n",
    "data_quality_report_categorical['2nd Mode in %'] = (df[categorical_features].apply(lambda x: x.value_counts().iloc[1] if len(x.value_counts()) > 1 else 'N/A') / len(df)) * 100\n",
    "\n",
    "data_quality_report_categorical['3rd Mode'] = df[categorical_features].apply(lambda x: x.value_counts().index[2] if len(x.value_counts()) > 1 else 'N/A')\n",
    "\n",
    "data_quality_report_categorical['3rd Mode Frequency'] = df[categorical_features].apply(lambda x: x.value_counts().iloc[2] if len(x.value_counts()) > 1 else 'N/A')\n",
    "\n",
    "data_quality_report_categorical['3rd Mode in %'] = (df[categorical_features].apply(lambda x: x.value_counts().iloc[2] if len(x.value_counts()) > 1 else 'N/A') / len(df)) * 100\n",
    "\n",
    "data_quality_report_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5b97fd-19db-4786-bce3-7258f03d5ae6",
   "metadata": {},
   "source": [
    "## 5. Initial Exploratory Data Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598152ab-6de0-49c3-80af-980aa7c9c278",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "selected_columns = [\n",
    "    'proportion_timeout', 'proportion_abort', 'proportion_unreachable',\n",
    "    'proportion_terminated', 'avg_dns_success_time', 'avg_dns_failure_time',\n",
    "    'count_dns_failure', 'ssl_error_prop', 'avg_tls_handshake_time'\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "for i, col in enumerate(selected_columns, 1):\n",
    "    plt.subplot(3, 3, i)\n",
    "    sns.histplot(df[col], bins=50, kde=True)\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "    if 'proportion' in col:\n",
    "        plt.xlim(0, df[col].max()) \n",
    "    elif 'avg' in col:\n",
    "        plt.xlim(0, df[col].max())\n",
    "    elif 'count' in col:\n",
    "        plt.xlim(0, df[col].max()) \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91ebf31-c7b1-43b2-ad05-4a22b1859e81",
   "metadata": {},
   "source": [
    "**Description**\n",
    "\n",
    "The histogram above demonstrates the ditribution of the continuous features present in the dataset. Here we can observe that most of the proportion values lie between 0 and 1 and they have some values higher than 1 which can possibly be outliers. However, avg_tls_handshake_time, avg_dns_success_time and avg_dns_failure_time have much higher values present the reason behind this is that they are recorded in milliseconds and could be tranformed, if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8031e90-423b-4749-adbd-14512c4b5a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion_features = [col for col in df.columns if \"proportion\" in col]\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "for i, feature in enumerate(proportion_features, 1):\n",
    "    plt.subplot(3, 3, i)\n",
    "    sns.boxplot(y=df[feature])\n",
    "    plt.ylim(-0.1, 1.1) \n",
    "    plt.title(feature)\n",
    "    plt.ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff903c7-36e1-4a35-ab63-a4b73d305912",
   "metadata": {},
   "source": [
    "**Description**\n",
    "\n",
    "The boxplots above visualizes the spread of data for proportion of the undefined, timeout, abort, unreachable, terminated, channel_open features respectively. They are necessary to understand our target feature as they're the set of features that captures the outcomes of the telemetry signals captured from the host machines. Here, we can observe that proportion_unreachable has the highest spread of values and also aligns with our target feature requirement as it captures the failure of upload of the telemetary signals indicating the possible outage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e326b043-0c4f-448e-b15e-8b1ad80a1d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12, 10))\n",
    "\n",
    "sns.scatterplot(data=df, x='proportion_unreachable', y='proportion_timeout', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Proportion Unreachable vs Timeout')\n",
    "\n",
    "sns.scatterplot(data=df, x='proportion_unreachable', y='proportion_terminated', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Proportion Unreachable vs Terminated')\n",
    "\n",
    "\n",
    "sns.scatterplot(data=df, x='proportion_unreachable', y='proportion_abort', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Proportion Unreachable vs Proportion Abort')\n",
    "\n",
    "sns.scatterplot(data=df, x='proportion_unreachable', y='proportion_channel_open', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Proportion Unreachable vs Proportion Channel Open')\n",
    "\n",
    "sns.scatterplot(data=df, x='proportion_unreachable', y='count_dns_failure', ax=axes[2,0])\n",
    "axes[2,0].set_title('Proportion Unreachable vs Count DNS Failure')\n",
    "\n",
    "sns.scatterplot(data=df, x='proportion_unreachable', y='avg_tls_handshake_time', ax=axes[2,1])\n",
    "axes[2,1].set_title('Proportion Unreachable vs Avg TLS Handshake Time')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03527d07-635b-4310-8afe-3463a9b15d71",
   "metadata": {},
   "source": [
    "**Description**\n",
    "\n",
    "The scatterplot above demonstrates the spread of the continuous features with respect to proportion_unreachable which is our target feature, this is to understand the correlation and the density of the values lying amongst the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa6250d-6820-4fec-aeda-f4c57f3851bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "df['country'].value_counts().head(15).plot(kind='bar', color='skyblue')\n",
    "plt.title('Top 15 Countries by Data Count')\n",
    "plt.xlabel('Country')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "df['city'].value_counts().head(15).plot(kind='bar', color='lightcoral')\n",
    "plt.title('Top 15 Cities by Data Count')\n",
    "plt.xlabel('City')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c29711-1f8f-42ce-9f63-f96bc31d02fc",
   "metadata": {},
   "source": [
    "**Description**\n",
    "\n",
    "The barplots above are used to demonstrate the top 15 countries and cities with highest data counts. It's quite significant that the most of the data is captures from United States followed by Germany, France and China. It's important to note that the cities have highest count for unknown as the cities with population less than 15,000 are labelled as unknows as originally mentioned by the data owners. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c384fe5b-317a-490b-8222-3b64904b3019",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_numeric_features = [\n",
    "    'proportion_timeout', 'proportion_abort', 'proportion_unreachable',\n",
    "    'proportion_terminated', 'avg_dns_success_time', 'avg_dns_failure_time',\n",
    "    'count_dns_failure', 'ssl_error_prop', 'avg_tls_handshake_time'\n",
    "]\n",
    "\n",
    "\n",
    "correlation_matrix = df[selected_numeric_features].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a223ec-68a3-4270-8ad9-2e709085ed7c",
   "metadata": {},
   "source": [
    "**Description**\n",
    "\n",
    "The heatmap above is used to understand the correlation between continuous features in the dataset. We can see that most of the features have positive correlation. However there are some features which have little negative correlation between them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333a43da-23c4-4b63-95d6-c67264f76de8",
   "metadata": {},
   "source": [
    "## 6. Data Cleaning & Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108f22c3-c6ab-45d3-84ab-5d477d74c88e",
   "metadata": {},
   "source": [
    "### Handling country with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96aa4dc-7f49-4c9c-b4a8-1f4e196f5547",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_null_country = df[df['country'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aff4e1a-1e0e-430c-8212-a4addfd1338c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_null_country['city'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b667ae-ac08-49ef-861c-8a09f4b79048",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_windhoek_count = df_null_country['city'].value_counts()['Windhoek']\n",
    "city_unknown_count =  df_null_country['city'].value_counts()['unknown']\n",
    "\n",
    "total = (city_windhoek_count + city_unknown_count)\n",
    "\n",
    "print('Windhoek count = {} and unknown count = {}'.format(city_windhoek_count, city_unknown_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6867ac8-9a6e-42f1-b88c-18cbbe434ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"city\"] == \"Windhoek\", \"country\"] = 'NA'\n",
    "df_null_country.loc[df_null_country[\"city\"] == \"Windhoek\", \"country\"] = 'NA'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1416541c-376d-4524-ae3f-9da98cb29222",
   "metadata": {},
   "source": [
    "**Description**\n",
    "\n",
    "Assigning the `country` value 'NA' i.e., Namibia where `city` is Windhoek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07ac9c0-396d-4793-9c46-956292d976e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74398465-7383-4627-9952-211b562dcac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_null_country[df_null_country['country'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f98749c-ddf5-420f-89bb-f86c7538089b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['country'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948e5366-70c2-4cdd-81cc-cf4229e1bf5c",
   "metadata": {},
   "source": [
    "**Description**\n",
    "\n",
    "Dropping all the remainder countries having null values as city is also unknown and the data count is significantly small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150dff66-47fe-4e3c-9294-af70a5d21c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff40c105-9694-49b3-a8dc-fa5a4dbdbece",
   "metadata": {},
   "source": [
    "### Handling all the null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d055d57-b4fc-4603-94ce-f1fa506f4851",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fde2fb6-13c6-48ac-b257-4e967e4a7e41",
   "metadata": {},
   "source": [
    "**Description**\n",
    "\n",
    "Dropping all the null values as the data count for null values is quite small.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcee73c-e4b2-4b8f-b857-b276ab0f4913",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0d6302-8acd-4752-9920-bdcf97700541",
   "metadata": {},
   "source": [
    "### Handling city with unknown values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4725af96-62ea-48e7-81bc-1b6a0ab1cca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_unknown_count =  df['city'].value_counts()['unknown']\n",
    "\n",
    "city_unknown_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ed5bcc-cb1c-4f87-b2c9-a902a37cd93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city_unknown = df[df['city'] == 'unknown']\n",
    "df_city_unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ea7b90",
   "metadata": {},
   "source": [
    "## 7. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4880675",
   "metadata": {},
   "source": [
    "### Define time slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6b60cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'datetime' to 'hour'\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "\n",
    "# Define time slots\n",
    "def get_detailed_time_slot(hour):\n",
    "    if 0 <= hour < 6:\n",
    "        return 'Late Night'\n",
    "    elif 6 <= hour < 9:\n",
    "        return 'Early Morning'\n",
    "    elif 9 <= hour < 12:\n",
    "        return 'Late Morning'\n",
    "    elif 12 <= hour < 15:\n",
    "        return 'Early Afternoon'\n",
    "    elif 15 <= hour < 18:\n",
    "        return 'Late Afternoon'\n",
    "    elif 18 <= hour < 21:\n",
    "        return 'Early Evening'\n",
    "    else:\n",
    "        return 'Night'\n",
    "\n",
    "df['time_slot'] = df['hour'].apply(get_detailed_time_slot) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230e969d",
   "metadata": {},
   "source": [
    "### Class Labeling (Data discretization) using composite score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45eb98cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example features that might contribute to an internet quality score\n",
    "features = ['proportion_timeout', 'proportion_unreachable', 'proportion_terminated', \n",
    "            'avg_dns_failure_time', 'count_dns_failure']\n",
    "\n",
    "# Create a composite score as a simple sum of standardized features\n",
    "df['composite_score'] = df[features].apply(lambda x: (x - x.mean()) / x.std()).sum(axis=1)\n",
    "\n",
    "# Calculate the quantiles on this composite score\n",
    "quantiles = df['composite_score'].quantile([0.25, 0.5, 0.75])\n",
    "\n",
    "# Define the labeling function with the correct quartile values\n",
    "def label_quality(score, quantiles):\n",
    "    if score <= quantiles[0.25]:\n",
    "        return 'good'\n",
    "    elif score <= quantiles[0.50]:\n",
    "        return 'moderate'\n",
    "    elif score <= quantiles[0.75]:\n",
    "        return 'bad'\n",
    "    else:\n",
    "        return 'worse'\n",
    "\n",
    "# Apply the labeling function to each row in your dataframe\n",
    "df['quality_label'] = df['composite_score'].apply(label_quality, quantiles=quantiles)\n",
    "\n",
    "# Map the categorical labels to integers\n",
    "label_map = {'good': 0, 'moderate': 1, 'bad': 2, 'worse': 3}\n",
    "df['quality_label_encoded'] = df['quality_label'].map(label_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7aeb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c074f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe34907",
   "metadata": {},
   "source": [
    "### Checking feature importance using RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4228e163",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['quality_label', 'quality_label_encoded', 'datetime','time_slot', 'country', 'city'], axis=1)\n",
    "y = df['quality_label_encoded']\n",
    "\n",
    "# Create the RFE object and rank each pixel\n",
    "rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=10)\n",
    "rfe.fit(X, y)\n",
    "\n",
    "# RFE ranking\n",
    "ranking_rfe = rfe.ranking_\n",
    "\n",
    "# To map these rankings back to column names:\n",
    "rfe_dict = dict(zip(X.columns, ranking_rfe))\n",
    "sorted_rfe = sorted(rfe_dict.items(), key=lambda item: item[1])\n",
    "\n",
    "# sorted_rfe now contains features and their RFE ranking, sorted from most to least important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d63a335",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted_rfe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d6f670",
   "metadata": {},
   "source": [
    "### Encoding the categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa3c8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encode 'country' and 'city'\n",
    "label_encoder_country = LabelEncoder()\n",
    "label_encoder_city = LabelEncoder()\n",
    "ordinal_encoder_time_slot = OrdinalEncoder()\n",
    "\n",
    "# Assuming 'data' is your DataFrame\n",
    "df['country_encoded'] = label_encoder_country.fit_transform(df['country'])\n",
    "df['city_encoded'] = label_encoder_city.fit_transform(df['city'])\n",
    "\n",
    "# Assuming 'time_slot' is a categorical variable that you want to encode ordinally\n",
    "df['time_slot_encoded'] = ordinal_encoder_time_slot.fit_transform(df[['time_slot']])\n",
    "\n",
    "# For 'composite_score', first, we need to convert it into quartile bins\n",
    "# Then we'll use ordinal encoding on these bins\n",
    "#data['quality_label_encoded'] = pd.qcut(data['quality_label'], q=4, labels=False)\n",
    "\n",
    "# Drop the original columns that have been encoded\n",
    "df.drop(['country', 'city', 'time_slot', 'quality_label'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937da151",
   "metadata": {},
   "source": [
    "## 8. Splitting the dataset into test, train and validate  sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0841231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features are separated: numerical features that need scaling and categorical encoded features that don't\n",
    "numerical_features = ['proportion_timeout', 'proportion_unreachable', 'proportion_terminated', \n",
    "                      'avg_dns_success_time', 'avg_dns_failure_time', 'count_dns_failure', 'ssl_error_prop']\n",
    "categorical_features = ['country_encoded', 'city_encoded', 'time_slot_encoded']\n",
    "X_numerical = df[numerical_features]\n",
    "X_categorical = df[categorical_features]\n",
    "\n",
    "# Target variable\n",
    "y = df['quality_label_encoded']\n",
    "\n",
    "# Split the data into train+validate and test sets (90-10 split)\n",
    "X_temp_num, X_test_num, y_temp, y_test = train_test_split(X_numerical, y, test_size=0.1, stratify=y, random_state=42, shuffle=True)\n",
    "X_temp_cat, X_test_cat = train_test_split(X_categorical, test_size=0.1, random_state=42, shuffle=True)\n",
    "\n",
    "# Further split the train+validate into train and validate sets (89-11 split, approximates to 80-10 of original)\n",
    "X_train_num, X_validate_num, y_train, y_validate = train_test_split(X_temp_num, y_temp, test_size=1/9, stratify=y_temp, random_state=42, shuffle=True)\n",
    "X_train_cat, X_validate_cat = train_test_split(X_temp_cat, test_size=1/9, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbed290b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fecaf7",
   "metadata": {},
   "source": [
    "### Initialize the StandardScaler for numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfc1043",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the numerical part of the training data and transform\n",
    "X_train_num_scaled = scaler.fit_transform(X_train_num)\n",
    "X_validate_num_scaled = scaler.transform(X_validate_num)\n",
    "X_test_num_scaled = scaler.transform(X_test_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f224f8e0",
   "metadata": {},
   "source": [
    "### Combining scaled continous and categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7310f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((X_train_num_scaled, X_train_cat.values), axis=1)\n",
    "X_validate = np.concatenate((X_validate_num_scaled, X_validate_cat.values), axis=1)\n",
    "X_test = np.concatenate((X_test_num_scaled, X_test_cat.values), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8345283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Continous features scaled shape', X_train_num_scaled.shape)\n",
    "print('Categorical features shape', X_train_cat.shape)\n",
    "print('X train shape', X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ddb1d3",
   "metadata": {},
   "source": [
    "### Checking the count of each class in the target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6ccea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution in the target feature\n",
    "class_counts = y_train.value_counts()\n",
    "print(class_counts)\n",
    "\n",
    "# Decide to use SMOTE based on class distribution\n",
    "# Generally, if any class is less than 10-20% of the majority class, SMOTE might be useful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe4dfd0",
   "metadata": {},
   "source": [
    "### Applying SMOTE to the training set if the classes are imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4643b716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SMOTE and resample the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bda6593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert to pandas Series for easy value counts (if not already a Series)\n",
    "y_train_smote_series = pd.Series(y_train_smote)\n",
    "label_distribution = y_train_smote_series.value_counts()\n",
    "print(label_distribution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f666e6",
   "metadata": {},
   "source": [
    "## 9. Modeling using Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4acaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming X_train_smote, y_train_smote, X_validate, y_validate are already defined and scaled\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f437ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define C values\n",
    "C_values = [0.1]\n",
    "train_accuracies = []\n",
    "validation_accuracies = []\n",
    "\n",
    "# Loop over the C values\n",
    "for C_value in C_values:\n",
    "    svm_model = LinearSVC(C=C_value, random_state=42)\n",
    "    svm_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "    # Training accuracy\n",
    "    y_train_pred = svm_model.predict(X_train_smote)\n",
    "    train_accuracy = accuracy_score(y_train_smote, y_train_pred)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    # Validation accuracy\n",
    "    y_val_pred = svm_model.predict(X_validate)\n",
    "    val_accuracy = accuracy_score(y_validate, y_val_pred)\n",
    "    validation_accuracies.append(val_accuracy)\n",
    "\n",
    "print(f\"Accuracy for C={C_value}: Training = {train_accuracy * 100:.2f}%, Validation = {val_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826b45e9",
   "metadata": {},
   "source": [
    "## 10. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162397e5",
   "metadata": {},
   "source": [
    "###  Plotting accuracies for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb5b55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting accuracies\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(C_values, train_accuracies, label='Training Accuracy', marker='o')\n",
    "plt.plot(C_values, validation_accuracies, label='Validation Accuracy', marker='o')\n",
    "plt.title('SVM Training and Validation Accuracy vs. C Parameter')\n",
    "plt.xlabel('C Parameter')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.xscale('log')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfaa725",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a388713",
   "metadata": {},
   "outputs": [],
   "source": [
    "for C_value in C_values:\n",
    "    svm_model = LinearSVC(C=C_value, random_state=42)\n",
    "    svm_model.fit(X_train_smote, y_train_smote)\n",
    "    y_val_pred = svm_model.predict(X_validate)\n",
    "    cm = confusion_matrix(y_validate, y_val_pred)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.title(f'Confusion Matrix (SVM) - C={C_value}')\n",
    "    plt.ylabel('Actual Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedbcf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and plot the confusion matrix\n",
    "cm = confusion_matrix(y_validate, y_pred_validate)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title('Confusion Matrix (SVM)')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53459f34",
   "metadata": {},
   "source": [
    "### Classification Report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476f13ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate a classification report\n",
    "report = classification_report(y_validate, y_pred_validate)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514971fc",
   "metadata": {},
   "source": [
    "### Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff300489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the SVM model to a file\n",
    "joblib.dump(svm_model, 'svm_model.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06b2e17",
   "metadata": {},
   "source": [
    "### Loading the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fc7a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SVM model from the file\n",
    "loaded_svm_model = joblib.load('svm_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47888bbb",
   "metadata": {},
   "source": [
    "### Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0160bc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred_test = loaded_svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate the predictions\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
